{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976e9a55-af53-4a1c-bcf8-11f64a7d4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea5c934-45cb-4562-af22-c8518e736c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./all_sentences.json') as json_file:\n",
    "    sentence_list_words = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6fa74b-a43a-4577-9b94-2b416b6c0540",
   "metadata": {},
   "source": [
    "Bag Of Words Feature Vector Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6303208a-11fa-48d5-97c1-b96c1618ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for sentence in sentence_list_words[\"sentences\"]:\n",
    "    for word in sentence:\n",
    "        all_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2be8edd-2015-4698-b395-97d54af11c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector_bag_of_words(sentence_words, set_of_words):\n",
    "    all_words_dict = {}\n",
    "    for word in set_of_words:\n",
    "        all_words_dict[word] = 0\n",
    "    \n",
    "    for word in sentence_words:\n",
    "        all_words_dict[word] = 1\n",
    "        \n",
    "    final_vector = []\n",
    "    for key in all_words_dict:\n",
    "        final_vector.append(all_words_dict[key])\n",
    "    \n",
    "    return final_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3f62c7-c393-405a-a8b8-9a0a74404acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_bag_of_words = {}\n",
    "for i in range(len(sentence_list_words[\"sentences\"])):\n",
    "    final_data_bag_of_words[i] = []\n",
    "    final_data_bag_of_words[i].append(sentence_list_words[\"sentences\"][i])\n",
    "    full_sentence = \"\"\n",
    "    for word in sentence_list_words[\"sentences\"][i]:\n",
    "        full_sentence += word\n",
    "        full_sentence += \" \"\n",
    "    \n",
    "    full_sentence = full_sentence[:-1]\n",
    "    final_data_bag_of_words[i].append(full_sentence)\n",
    "    bag_of_words_vector = make_vector_bag_of_words(sentence_list_words[\"sentences\"][i], all_words)\n",
    "    final_data_bag_of_words[i].append(bag_of_words_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1a15b-605a-462c-9732-58c87fe8da18",
   "metadata": {},
   "source": [
    "Count Vectorizer Approach Feature Vector Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9734e35e-c9aa-4ad0-b8fb-aaf189482e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector_count_vectorization(sentence_words, set_of_words):\n",
    "    all_words_dict = {}\n",
    "    for word in set_of_words:\n",
    "        all_words_dict[word] = 0\n",
    "    \n",
    "    for word in sentence_words:\n",
    "        all_words_dict[word] += 1\n",
    "        \n",
    "    final_vector = []\n",
    "    for key in all_words_dict:\n",
    "        final_vector.append(all_words_dict[key])\n",
    "    \n",
    "    return final_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b731c2e-048c-459c-b45e-95fbf39b6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_count_vectorization = {}\n",
    "for i in range(len(sentence_list_words[\"sentences\"])):\n",
    "    final_data_count_vectorization[i] = []\n",
    "    final_data_count_vectorization[i].append(sentence_list_words[\"sentences\"][i])\n",
    "    full_sentence = \"\"\n",
    "    for word in sentence_list_words[\"sentences\"][i]:\n",
    "        full_sentence += word\n",
    "        full_sentence += \" \"\n",
    "    \n",
    "    full_sentence = full_sentence[:-1]\n",
    "    final_data_count_vectorization[i].append(full_sentence)\n",
    "    count_vectorization_vector = make_vector_count_vectorization(sentence_list_words[\"sentences\"][i], all_words)\n",
    "    final_data_count_vectorization[i].append(count_vectorization_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d1589-36a8-4ba0-b619-780ba59c8e75",
   "metadata": {},
   "source": [
    "Feature Vector generation using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0790df71-a483-4be4-b289-7bf19d6145a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b24fbb-6c47-430a-b9d9-88e796e46824",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentence_list_words[\"sentences\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521a075-b3f2-465c-8521-978c7bc5e8ec",
   "metadata": {},
   "source": [
    "Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565de1d7-d7db-4361-86e2-79b01382e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tot_sum_clustered_sentences = {}\n",
    "for i in range(len(sentence_list_words[\"sentences\"])):\n",
    "    final_data_w2v_avg[i] = []\n",
    "    final_data_w2v_avg[i].append(sentence_list_words[\"sentences\"][i])\n",
    "    full_sentence = \"\"\n",
    "    for word in sentence_list_words[\"sentences\"][i]:\n",
    "        full_sentence += word\n",
    "        full_sentence += \" \"\n",
    "    \n",
    "    full_sentence = full_sentence[:-1]\n",
    "    final_data_w2v_avg[i].append(full_sentence)\n",
    "    \n",
    "    \n",
    "    \n",
    "for key in final_data_bag_of_words:\n",
    "    vectors_of_words_in_sentece = []\n",
    "    for word in final_data_bag_of_words[key][0]:\n",
    "        vectors_of_words_in_sentece.append(model.wv[word])\n",
    "        \n",
    "    avg = np.mean(vectors_of_words_in_sentece, axis = 0)\n",
    "    final_data_w2v_avg[key].append(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39f93b-5f9d-4b04-986e-646313dd094e",
   "metadata": {},
   "source": [
    "Total Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7863a9-4619-4033-8195-72171b8c57a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_w2v_tot_sum = {}\n",
    "for i in range(len(sentence_list_words[\"sentences\"])):\n",
    "    final_data_w2v_tot_sum[i] = []\n",
    "    final_data_w2v_tot_sum[i].append(sentence_list_words[\"sentences\"][i])\n",
    "    full_sentence = \"\"\n",
    "    for word in sentence_list_words[\"sentences\"][i]:\n",
    "        full_sentence += word\n",
    "        full_sentence += \" \"\n",
    "    \n",
    "    full_sentence = full_sentence[:-1]\n",
    "    final_data_w2v_tot_sum[i].append(full_sentence)\n",
    "    \n",
    "    \n",
    "    \n",
    "for key in final_data_bag_of_words:\n",
    "    vectors_of_words_in_sentece = []\n",
    "    for word in final_data_bag_of_words[key][0]:\n",
    "        vectors_of_words_in_sentece.append(model.wv[word])\n",
    "        \n",
    "    avg = np.sum(vectors_of_words_in_sentece, axis = 0)\n",
    "    final_data_w2v_tot_sum[key].append(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cd539-1e6b-4bfc-8e6f-57a26945a035",
   "metadata": {},
   "source": [
    "Clustering With hdbcan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3275495-c1f9-476c-bf28-48cb256b478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0522fc-8624-4301-b5d9-f1050a404c07",
   "metadata": {},
   "source": [
    "K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dda888-fa6c-444e-acd6-b3ae6f2e43d4",
   "metadata": {},
   "source": [
    "Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "99765af6-7da0-459b-96b9-6a8e52855172",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_vectors = []\n",
    "for key in final_data_bag_of_words:\n",
    "    bag_of_words_vectors.append(final_data_bag_of_words[key][2])\n",
    "    \n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=6, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(bag_of_words_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b81af7b-9563-4ca3-a517-9b6bc783ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_clustered_sentences = {}\n",
    "for n in y_km:\n",
    "    bag_of_words_clustered_sentences[int(n)] = []\n",
    "    \n",
    "for n in range(len(y_km)):\n",
    "    cur_data_point = [final_data_bag_of_words[n][0],final_data_bag_of_words[n][1]]\n",
    "    bag_of_words_clustered_sentences[y_km[n]].append(cur_data_point)\n",
    "    \n",
    "with open(\"./k_means/bag_of_words_clustered_sentences.json\", \"w\") as outfile:\n",
    "    json.dump(bag_of_words_clustered_sentences, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dc432-83bb-4c74-82a7-40903230937c",
   "metadata": {},
   "source": [
    "Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cfe6be55-b30b-4be5-a910-07dc12da1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_vectors = []\n",
    "for key in final_data_count_vectorization:\n",
    "    count_vectorizer_vectors.append(final_data_count_vectorization[key][2])\n",
    "    \n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=7, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(count_vectorizer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52e165f0-c36d-402f-b304-7607c1686d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_clustered_sentences = {}\n",
    "for n in y_km:\n",
    "    count_vectorizer_clustered_sentences[int(n)] = []\n",
    "    \n",
    "for n in range(len(y_km)):\n",
    "    cur_data_point = [final_data_count_vectorization[n][0],final_data_count_vectorization[n][1]]\n",
    "    count_vectorizer_clustered_sentences[y_km[n]].append(cur_data_point)\n",
    "    \n",
    "with open(\"./k_means/count_vectorizer_clustered_sentences.json\", \"w\") as outfile:\n",
    "    json.dump(count_vectorizer_clustered_sentences, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3964f2-1525-4fb2-b5fa-b44b4da9acfe",
   "metadata": {},
   "source": [
    "Word 2 Vec Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19c41688-8f8a-4ce7-8065-f986892f9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors_avg = []\n",
    "for key in final_data_w2v_avg:\n",
    "    w2v_vectors_avg.append(final_data_w2v_avg[key][2])\n",
    "    \n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=7, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(w2v_vectors_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78f8192e-0330-4dc7-a91a-ac10550207cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_avg_clustered_sentences = {}\n",
    "for n in y_km:\n",
    "    w2v_avg_clustered_sentences[int(n)] = []\n",
    "    \n",
    "for n in range(len(y_km)):\n",
    "    cur_data_point = [final_data_w2v_avg[n][0],final_data_w2v_avg[n][1]]\n",
    "    w2v_avg_clustered_sentences[y_km[n]].append(cur_data_point)\n",
    "    \n",
    "with open(\"./k_means/w2v_avg_clustered_sentences.json\", \"w\") as outfile:\n",
    "    json.dump(w2v_avg_clustered_sentences, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d04845d2-0a34-4845-b51b-0ed8ad7030f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vectors_tot_sum = []\n",
    "for key in final_data_w2v_tot_sum:\n",
    "    w2v_vectors_tot_sum.append(final_data_w2v_tot_sum[key][2])\n",
    "    \n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=7, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(w2v_vectors_tot_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe418e7b-d7e6-4d6d-933f-255f4ac35271",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tot_sum_clustered_sentences = {}\n",
    "for n in y_km:\n",
    "    w2v_tot_sum_clustered_sentences[int(n)] = []\n",
    "    \n",
    "for n in range(len(y_km)):\n",
    "    cur_data_point = [final_data_w2v_tot_sum[n][0],final_data_w2v_tot_sum[n][1]]\n",
    "    w2v_tot_sum_clustered_sentences[y_km[n]].append(final_data_w2v_tot_sum[n][1])\n",
    "    \n",
    "with open(\"./k_means/w2v_tot_sum_clustered_sentences.json\", \"w\") as outfile:\n",
    "    json.dump(w2v_tot_sum_clustered_sentences, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c5c80-65bd-4c72-aacb-6ba44e592706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
